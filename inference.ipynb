{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "from utils.loading_utils import load_model, get_device\n",
    "import argparse\n",
    "from utils.event_readers import FixedSizeEventReader, FixedDurationEventReader\n",
    "from utils.inference_utils import events_to_voxel_grid, events_to_voxel_grid_pytorch\n",
    "import numpy as np\n",
    "from utils.timers import Timer\n",
    "from image_reconstructor import ImageReconstructor\n",
    "from options.inference_options import set_inference_options\n",
    "\n",
    "\n",
    "def main_inference_options():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluating a trained network\")\n",
    "    parser.add_argument(\n",
    "        \"-c\", \"--path_to_model\", required=False, type=str, help=\"path to model weights\"\n",
    "    )\n",
    "    parser.add_argument(\"-i\", \"--input_file\", required=False, type=str)\n",
    "    parser.add_argument(\"--fixed_duration\", dest=\"fixed_duration\", action=\"store_true\")\n",
    "    parser.set_defaults(fixed_duration=False)\n",
    "    parser.add_argument(\n",
    "        \"-N\",\n",
    "        \"--window_size\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"Size of each event window, in number of events. Ignored if --fixed_duration=True\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-T\",\n",
    "        \"--window_duration\",\n",
    "        default=33.33,\n",
    "        type=float,\n",
    "        help=\"Duration of each event window, in milliseconds. Ignored if --fixed_duration=False\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_events_per_pixel\",\n",
    "        default=0.35,\n",
    "        type=float,\n",
    "        help=\"in case N (window size) is not specified, it will be \\\n",
    "                              automatically computed as N = width * height * num_events_per_pixel\",\n",
    "    )\n",
    "    parser.add_argument(\"--skipevents\", default=0, type=int)\n",
    "    parser.add_argument(\"--suboffset\", default=0, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--compute_voxel_grid_on_cpu\",\n",
    "        dest=\"compute_voxel_grid_on_cpu\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.set_defaults(compute_voxel_grid_on_cpu=False)\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite data reader to avoid file IO and $O(n^2)$ event window iterations in original code of E2VID, or code infinitely hangs up on large GOPRO datasets. No changes to logics and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.timers import Timer\n",
    "\n",
    "\n",
    "class MyFixedSizeEventReader:\n",
    "    \"\"\"\n",
    "    Reads events from a '.h5' file, and packages the events into\n",
    "    non-overlapping event windows, each containing a fixed number of events.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, events, num_events=10000, start_index=0):\n",
    "        print(\"Will use fixed size event windows with {} events\".format(num_events))\n",
    "        print(\"Output frame rate: variable\")\n",
    "        self.events = events[start_index:]\n",
    "        self.num_events = num_events\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.events) > self.num_events:\n",
    "            event_window = self.events[: self.num_events]\n",
    "            self.events = self.events[self.num_events :]\n",
    "        else:\n",
    "            event_window = self.events[:]\n",
    "            self.events = []\n",
    "        event_window = np.array(event_window)\n",
    "        return event_window\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyFixedDurationEventReader:\n",
    "    \"\"\"\n",
    "    Reads events from a '.h5' file, and packages the events into\n",
    "    non-overlapping event windows, each of a fixed duration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, events, duration_ms=50.0, start_index=0):\n",
    "        print(\n",
    "            \"Will use fixed duration event windows of size {:.2f} ms\".format(\n",
    "                duration_ms\n",
    "            )\n",
    "        )\n",
    "        print(\"Output frame rate: {:.1f} Hz\".format(1000.0 / duration_ms))\n",
    "        self.events = np.array(events[start_index:])\n",
    "        self.events = self.events[\n",
    "            self.events[:, 0].argsort()\n",
    "        ]  # Sort the events based on time\n",
    "        self.duration_s = duration_ms / 1000.0\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __del__(self):\n",
    "        self.event_file.close()\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= len(self.events):\n",
    "            raise StopIteration\n",
    "        start_time = self.events[self.current_index][0]\n",
    "        end_index = self.events[:, 0].searchsorted(\n",
    "            start_time + self.duration_s, side=\"right\"\n",
    "        )\n",
    "        event_list = self.events[self.current_index : end_index]\n",
    "        self.current_index = end_index\n",
    "        return event_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic reader for alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruthTimestampEventReader:\n",
    "    \"\"\"\n",
    "    Reads events from a '.h5' file, and packages the events into\n",
    "    event windows whose start and end times align with the timestamps of the ground truth images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, events, ground_truth_timestamps, start_index=0):\n",
    "        self.events = np.array(events[start_index:])\n",
    "        self.events = self.events[self.events[:, 0].argsort()]  # Sort the events based on time\n",
    "        self.ground_truth_timestamps = np.array(ground_truth_timestamps) / 1e9\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= len(self.ground_truth_timestamps):\n",
    "            raise StopIteration\n",
    "        start_time = self.ground_truth_timestamps[self.current_index]\n",
    "        end_time = self.ground_truth_timestamps[self.current_index + 1] if self.current_index + 1 < len(self.ground_truth_timestamps) else self.events[-1, 0]\n",
    "        start_index = self.events[:, 0].searchsorted(start_time, side='left')\n",
    "        end_index = self.events[:, 0].searchsorted(end_time, side='right')\n",
    "        event_list = self.events[start_index:end_index]\n",
    "        self.current_index += 1\n",
    "        return event_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def convert_h5_to_df(h5_file_path):\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        events = f['events']\n",
    "        xs = events['xs'][:]\n",
    "        ys = events['ys'][:]\n",
    "        ts = events['ts'][:]\n",
    "        ps = events['ps'][:]\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        't': (ts / 1e9).astype(np.float64), # In seconds\n",
    "        'x': xs.astype(np.int16), \n",
    "        'y': ys.astype(np.int16),\n",
    "        'pol': ps.astype(np.int16)\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_h5_to_txt(h5_file_path, txt_file_path):\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        events = f['events']\n",
    "        xs = events['xs'][:]\n",
    "        ys = events['ys'][:]\n",
    "        ts = events['ts'][:]\n",
    "        ps = events['ps'][:]\n",
    "        height, width = f.attrs['sensor_resolution'][0:2]\n",
    "    \n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'ts': ts,\n",
    "        'xs': xs,\n",
    "        'ys': ys,\n",
    "        'ps': ps\n",
    "    })\n",
    "\n",
    "    with open(txt_file_path, 'w') as txt_file:\n",
    "        print(f\"Writing to {txt_file_path}\")\n",
    "        # Write header\n",
    "        txt_file.write(f\"{width} {height}\\n\")  \n",
    "    \n",
    "    # Append DataFrame contents to the file (after the header)\n",
    "    df.to_csv(txt_file_path, sep=' ', index=False, header=False, mode='a')\n",
    "\n",
    "\n",
    "TXT_STRUCTURE = \"ptxy\"  # Set this variable according to your requirement\n",
    "\n",
    "def convert_txt_to_df(txt_file_path):\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data = {'t': [], 'x': [], 'y': [], 'pol': []}\n",
    "    for line in lines[1:]:  # Ignore the first line which is the sensor info\n",
    "        parts = line.split()\n",
    "\n",
    "        # Depending on your data structure 'txyp' or 'pxyt'\n",
    "        if TXT_STRUCTURE == 'txyp':\n",
    "            data['t'].append(float(parts[0]))\n",
    "            data['x'].append(int(parts[1]))\n",
    "            data['y'].append(int(parts[2]))\n",
    "            data['pol'].append(int(parts[3]))\n",
    "        elif TXT_STRUCTURE == 'pxyt':\n",
    "            data['t'].append(float(parts[1]))\n",
    "            data['x'].append(int(parts[2]))\n",
    "            data['y'].append(int(parts[3]))\n",
    "            data['pol'].append(int(parts[0]))\n",
    "        elif TXT_STRUCTURE == 'ptxy':\n",
    "            data['t'].append(float(parts[1]))\n",
    "            data['x'].append(int(parts[2]))\n",
    "            data['y'].append(int(parts[3]))\n",
    "            data['pol'].append(int(parts[0]))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown TXT_STRUCTURE: {TXT_STRUCTURE}\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        't': np.array(data['t'], dtype=np.float64),\n",
    "        'x': np.array(data['x'], dtype=np.int16),\n",
    "        'y': np.array(data['y'], dtype=np.int16),\n",
    "        'pol': np.array(data['pol'], dtype=np.int16)\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_COLORED = False\n",
    "\n",
    "\n",
    "def extract_gt_images(h5_file_path, output_folder):\n",
    "    \"\"\"\n",
    "    Extracts ground truth images from an h5 file and saves them to a specified output folder.\n",
    "\n",
    "    Args:\n",
    "    - h5_file_path (str): The path to the h5 file containing the ground truth images.\n",
    "    - output_folder (str): The path to the folder where the extracted images will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - avg_interval (float): The average time interval between the extracted images.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting ground truth images from {h5_file_path} to {output_folder}\")\n",
    "    # Create gt folder if it doesn't exist\n",
    "    gt_folder = os.path.join(output_folder, 'gt')\n",
    "    os.makedirs(gt_folder, exist_ok=True)\n",
    "    \n",
    "    timestamps = [] # In ns\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        images_group = f['sharp_images']\n",
    "        for image_name in images_group:\n",
    "            image = images_group[image_name][:]\n",
    "\n",
    "            timestamp = images_group[image_name].attrs['timestamp']\n",
    "            timestamps.append(timestamp)  # store timestamps\n",
    "\n",
    "            image_path = os.path.join(gt_folder, f\"image_{timestamp:.0f}.png\")\n",
    "            if EXTRACT_COLORED:\n",
    "                cv2.imwrite(image_path, image)\n",
    "            else:\n",
    "                cv2.imwrite(image_path, image[:,:,0])\n",
    "    \n",
    "    timestamps = np.array(timestamps)\n",
    "    intervals = np.diff(timestamps)\n",
    "    avg_interval = np.mean(intervals) / 1e6  # in ms\n",
    "    std_interval = np.std(intervals) / 1e6  # in ms\n",
    "\n",
    "    # Save timestamps to a timestamps.txt in seconds, each line is a timestamp\n",
    "    timestamps = timestamps / 1e9  # in seconds\n",
    "    timestamps_path = os.path.join(gt_folder, 'timestamps.txt')\n",
    "    np.savetxt(timestamps_path, timestamps, fmt='%.10f')\n",
    "\n",
    "    return avg_interval, std_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DYNAMIC_WINDOWING = True\n",
    "def process_events(args):\n",
    "    if args.width and args.height:  # sensor size is provided in args\n",
    "        width = args.width\n",
    "        height = args.height\n",
    "    else:  # read sensor size from the first line of the event file\n",
    "        path_to_events = args.input_file\n",
    "        # Raise an error if the file does not exist\n",
    "        if not os.path.exists(path_to_events):\n",
    "            raise ValueError(f\"File {path_to_events} does not exist\")\n",
    "        \n",
    "        # Determine the type of the file\n",
    "        _, file_extension = os.path.splitext(args.input_file)\n",
    "        if file_extension == '.txt':\n",
    "            header = pd.read_csv(path_to_events, delim_whitespace=True, header=None, names=['width', 'height'],\n",
    "                                dtype={'width': np.int, 'height': np.int},\n",
    "                                nrows=1)\n",
    "            width, height = header.values[0]\n",
    "        elif file_extension == '.h5':\n",
    "            with h5py.File(path_to_events, 'r') as f:\n",
    "                height, width = f.attrs['sensor_resolution'][0:2]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown file extension {file_extension}\")\n",
    "    \n",
    "        print('Sensor size: {} x {}'.format(width, height))\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(args.path_to_model)\n",
    "    device = get_device(args.use_gpu)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    reconstructor = ImageReconstructor(model, height, width, model.num_bins, args)\n",
    "\n",
    "    N = args.window_size\n",
    "    if not args.fixed_duration:\n",
    "        if N is None:\n",
    "            N = int(width * height * args.num_events_per_pixel)\n",
    "            print('Will use {} events per tensor (automatically estimated with num_events_per_pixel={:0.2f}).'.format(\n",
    "                N, args.num_events_per_pixel))\n",
    "        else:\n",
    "            print('Will use {} events per tensor (user-specified)'.format(N))\n",
    "            mean_num_events_per_pixel = float(N) / float(width * height)\n",
    "            if mean_num_events_per_pixel < 0.1:\n",
    "                print('!!Warning!! the number of events used ({}) seems to be low compared to the sensor size. \\\n",
    "                    The reconstruction results might be suboptimal.'.format(N))\n",
    "            elif mean_num_events_per_pixel > 1.5:\n",
    "                print('!!Warning!! the number of events used ({}) seems to be high compared to the sensor size. \\\n",
    "                    The reconstruction results might be suboptimal.'.format(N))\n",
    "\n",
    "    initial_offset = args.skipevents\n",
    "    sub_offset = args.suboffset\n",
    "    start_index = initial_offset + sub_offset\n",
    "\n",
    "    if args.compute_voxel_grid_on_cpu:\n",
    "        print('Will compute voxel grid on CPU.')\n",
    "\n",
    "    if USE_DYNAMIC_WINDOWING:\n",
    "        timestamps_path = os.path.join(args.output_folder, 'gt', 'timestamps.txt')\n",
    "        timestamps = np.loadtxt(timestamps_path)\n",
    "        timestamps = timestamps * 1e9  # Convert to ns\n",
    "        if file_extension == '.h5':\n",
    "            df = convert_h5_to_df(args.input_file)\n",
    "        elif file_extension == '.txt':\n",
    "            df = convert_txt_to_df(args.input_file)\n",
    "        else:\n",
    "            raise ValueError(f\"Dynamic windowing is only supported for h5 files. Got {file_extension}\")\n",
    "        event_window_iterator = GroundTruthTimestampEventReader(df.values, timestamps)\n",
    "\n",
    "    else:\n",
    "        if args.fixed_duration:\n",
    "            if file_extension == '.h5':\n",
    "                # Convert h5 to pandas DataFrame\n",
    "                df = convert_h5_to_df(args.input_file)\n",
    "                event_window_iterator = MyFixedDurationEventReader(df.values,\n",
    "                                                                duration_ms=args.window_duration)\n",
    "            else:  # Assuming txt\n",
    "                event_window_iterator = FixedDurationEventReader(path_to_events,\n",
    "                                                                duration_ms=args.window_duration,\n",
    "                                                                start_index=start_index)\n",
    "        else:\n",
    "            if file_extension == '.h5':\n",
    "                # Convert h5 to pandas DataFrame\n",
    "                df = convert_h5_to_df(args.input_file)\n",
    "                event_window_iterator = MyFixedSizeEventReader(df.values, num_events=N)\n",
    "            else:  # Assuming txt\n",
    "                event_window_iterator = FixedSizeEventReader(path_to_events, num_events=N, start_index=start_index)\n",
    "\n",
    "    with Timer('Processing entire dataset'):\n",
    "        for event_window in event_window_iterator:\n",
    "            # Check if event_window is empty\n",
    "            if event_window.size == 0:\n",
    "                break\n",
    "            \n",
    "            last_timestamp = event_window[-1, 0]\n",
    "\n",
    "            with Timer('Building event tensor'):\n",
    "                if args.compute_voxel_grid_on_cpu:\n",
    "                    event_tensor = events_to_voxel_grid(event_window,\n",
    "                                                        num_bins=model.num_bins,\n",
    "                                                        width=width,\n",
    "                                                        height=height)\n",
    "                    event_tensor = torch.from_numpy(event_tensor)\n",
    "                else:\n",
    "                    event_tensor = events_to_voxel_grid_pytorch(event_window,\n",
    "                                                                num_bins=model.num_bins,\n",
    "                                                                width=width,\n",
    "                                                                height=height,\n",
    "                                                                device=device)\n",
    "\n",
    "            num_events_in_window = event_window.shape[0]\n",
    "            reconstructor.update_reconstruction(event_tensor, start_index + num_events_in_window, last_timestamp)\n",
    "\n",
    "            start_index += num_events_in_window\n",
    "    reconstructor.image_writer.timestamps_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def process_dataset(\n",
    "    dataset_dir, path_to_model, use_gpu, output_folder, use_resblur=False\n",
    "):\n",
    "    parser = main_inference_options()\n",
    "    set_inference_options(parser)\n",
    "    default_args = vars(parser.parse_args([]))  # get a dictionary of defaults\n",
    "    default_args.update(\n",
    "        {\n",
    "            \"use_gpu\": use_gpu,\n",
    "            \"auto_hdr\": True,\n",
    "            \"fixed_duration\": True,\n",
    "        }\n",
    "    )\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for split in os.listdir(dataset_dir):\n",
    "        split_path = os.path.join(dataset_dir, split)\n",
    "        if use_resblur:\n",
    "            pass\n",
    "            # if os.path.isdir(split_path):\n",
    "            #     # for folder in os.listdir(split_path):\n",
    "            #     #     folder_path = os.path.join(split_path, folder)\n",
    "            #     # Assert eventcntts_hhfcode_unnormed0606_txt and frame_clip folder exists, or AssertError\n",
    "            #     assert \"eventcntts_hhfcode_unnormed0606_txt\" in os.listdir(\n",
    "            #         split_path\n",
    "            #     ) and \"frame_clip\" in os.listdir(split_path)\n",
    "            #     events_path = os.path.join(\n",
    "            #         split_path, \"eventcntts_hhfcode_unnormed0606_txt\"\n",
    "            #     )\n",
    "            #     ground_truth_path = os.path.join(split_path, \"frame_clip\")\n",
    "            #     gt_path = os.path.join(output_folder, split, \"gt\")\n",
    "\n",
    "            #     # Read all txt by order of filename and concat into an object, later load into Dataframe\n",
    "            #     txt_files = sorted(glob.glob(os.path.join(events_path, \"*.txt\")))\n",
    "            #     concatenated = \"\"\n",
    "            #     for txt_file in txt_files:\n",
    "            #         with open(txt_file, \"r\") as f:\n",
    "            #             concatenated += f.read()\n",
    "\n",
    "            #     # Save the concatenated txt to a new txt file in split_path\n",
    "            #     concatenated_txt_path = os.path.join(split_path, \"concatenated.txt\")\n",
    "            #     with open(concatenated_txt_path, \"w\") as f:\n",
    "            #         f.write(concatenated)\n",
    "\n",
    "            #     # Parse all file names like [18707981-18710981]_000002.png, which means the start and end timestamps of the frame exposure in ns. Compute The average time interval between the extracted images.(Only start of exposure). And save the timestamps to a timestamps.txt in seconds, each line is a timestamp. And also save the image files to the gt folder, in the filename of image_{timestamp}.png\n",
    "            #     timestamps = []\n",
    "            #     for image_name in os.listdir(ground_truth_path):\n",
    "            #         image_path = os.path.join(ground_truth_path, image_name)\n",
    "            #         timestamp = int(image_name.split(\"_\")[0].split(\"-\")[0])\n",
    "            #         timestamps.append(timestamp)\n",
    "            #         image_path = os.path.join(gt_path, f\"image_{timestamp}.png\")\n",
    "\n",
    "            #         image = cv2.imread(image_path)\n",
    "            #         cv2.imwrite(image_path, image[:, :, 0])\n",
    "\n",
    "            #     timestamps = np.array(timestamps)\n",
    "            #     intervals = np.diff(timestamps)\n",
    "            #     avg_interval = np.mean(intervals) / 1e6  # in ms\n",
    "            #     std_interval = np.std(intervals) / 1e6  # in ms\n",
    "            #     print(f\"Average frame interval: {avg_interval}, std: {std_interval}\")\n",
    "\n",
    "        else:\n",
    "            # Check if subfolder is named train or test\n",
    "            if os.path.isdir(split_path) and split in [\"train\", \"test\"]:\n",
    "                for file in os.listdir(split_path):\n",
    "                    folder_path = os.path.join(split_path, file)\n",
    "                    subdir_output_folder = os.path.join(\n",
    "                        output_folder, split, file.split(\".\")[0]\n",
    "                    )\n",
    "                    os.makedirs(subdir_output_folder, exist_ok=True)\n",
    "\n",
    "                    if file.endswith(\".h5\"):  # convert .h5 files to .txt\n",
    "                        # print(f\"Converting {file_path} to .txt\")\n",
    "\n",
    "                        # txt_file_path = os.path.join(split_path, file.split('.')[0] + '.txt')\n",
    "                        # convert_h5_to_txt(file_path, txt_file_path)\n",
    "                        avg_frame_interval, std_frame_interval = extract_gt_images(\n",
    "                            folder_path, subdir_output_folder\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"Average frame interval: {avg_frame_interval}, std: {std_frame_interval}\"\n",
    "                        )\n",
    "                        # file_path = txt_file_path  # use the converted txt file\n",
    "\n",
    "                    elif not file.endswith(\".txt\"):  # skip non-txt files\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Processing {folder_path}\")\n",
    "\n",
    "                    args = default_args.copy()  # start with defaults\n",
    "                    # update with specific settings for this file\n",
    "                    args.update(\n",
    "                        {\n",
    "                            \"path_to_model\": path_to_model,\n",
    "                            \"input_file\": folder_path,\n",
    "                            \"use_gpu\": use_gpu,\n",
    "                            \"output_folder\": subdir_output_folder,\n",
    "                            \"window_duration\": avg_frame_interval,\n",
    "                        }\n",
    "                    )\n",
    "                    args_namespace = argparse.Namespace(**args)\n",
    "                    process_events(args_namespace)\n",
    "\n",
    "def get_sensor_size_from_image(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "    return width, height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resblur_scenario(scenario_path, path_to_model, use_gpu, output_folder):\n",
    "    parser = main_inference_options()\n",
    "    set_inference_options(parser)\n",
    "    default_args = vars(parser.parse_args([]))  # get a dictionary of defaults\n",
    "    default_args.update(\n",
    "        {\n",
    "            \"use_gpu\": use_gpu,\n",
    "            \"auto_hdr\": True,\n",
    "            \"fixed_duration\": True,\n",
    "        }\n",
    "    )\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    events_txt_path = os.path.join(scenario_path, 'events_txt')\n",
    "    frame_clip_path = os.path.join(scenario_path, 'frame_clip')\n",
    "    \n",
    "    # Get sensor size from a ground truth image\n",
    "    gt_image_path = next(glob.glob(os.path.join(frame_clip_path, '*.png')))\n",
    "    width, height = get_sensor_size_from_image(gt_image_path)\n",
    "    \n",
    "    # Concatenate all event text files into a DataFrame\n",
    "    txt_files = sorted(glob.glob(os.path.join(events_txt_path, '*.txt')))\n",
    "    events_df = pd.concat((convert_txt_to_df(f) for f in txt_files), ignore_index=True)\n",
    "    \n",
    "    # Process events\n",
    "    print(f\"Processing {scenario_path}\")\n",
    "    \n",
    "    subdir_output_folder = os.path.join(output_folder, os.path.basename(scenario_path))\n",
    "    os.makedirs(subdir_output_folder, exist_ok=True)\n",
    "\n",
    "    args = default_args.copy()  # start with defaults\n",
    "    # update with specific settings for this scenario\n",
    "    args.update(\n",
    "        {\n",
    "            \"path_to_model\": path_to_model,\n",
    "            \"input_file\": events_df,  # pass DataFrame directly\n",
    "            \"use_gpu\": use_gpu,\n",
    "            \"output_folder\": subdir_output_folder,\n",
    "            \"window_duration\": 33.33,  # you may need to adjust this value\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "        }\n",
    "    )\n",
    "    args_namespace = argparse.Namespace(**args)\n",
    "    process_events(args_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run data preparation and reconstruction in E2VID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For EFNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset_dir = \"/root/autodl-tmp/datasets/GOPRO_rawevents\"\n",
    "    path_to_model = '/root/autodl-tmp/rpg_e2vid/pretrained/E2VID_lightweight.pth.tar'\n",
    "    use_gpu = True\n",
    "    output_folder = \"/root/autodl-tmp/datasets/GOPRO_e2vid_pred\"\n",
    "\n",
    "    process_dataset(dataset_dir, path_to_model, use_gpu, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For RESBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset_dir = \"RESBLUR\"\n",
    "    path_to_model = '/root/autodl-tmp/rpg_e2vid/pretrained/E2VID_lightweight.pth.tar'\n",
    "    use_gpu = True\n",
    "    output_folder = \"RESBLUR_OUT\"\n",
    "\n",
    "    for scenario in os.listdir(dataset_dir):\n",
    "        scenario_path = os.path.join(dataset_dir, scenario)\n",
    "        if os.path.isdir(scenario_path):\n",
    "            process_resblur_scenario(scenario_path, path_to_model, use_gpu, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
